{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9304d6f4",
   "metadata": {},
   "source": [
    "Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2eb7c",
   "metadata": {},
   "source": [
    "NanoVLM: tiny CLIP-style model trained on synthetic colored-shaped captions.\n",
    "\n",
    "For a given text caption, we have to retrieve the best images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94436a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray']\n",
    "shapes = ['square', 'circle', 'triangle']\n",
    "positions = ['left', 'center', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42717e0f",
   "metadata": {},
   "source": [
    "This is a tiny CLIP-styled Vision language Model. It has 2 separate encoders- one for images and one for text. Both encoder map their inputs into a common embeddingspace of dimension 64 (or other dimension we choose). THe goal is to make matching image-text pairs lie close together in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b790ab",
   "metadata": {},
   "source": [
    "Image Encoder\n",
    "\n",
    "- A small CNN (4 convolutional layers) progressively downsamples the input image.\n",
    "- After the convolution blocks, a global average pooling layer reduces spatial features.\n",
    "- A linear projection maps to the embedding dimension.\n",
    "- Finally, a LayerNorm + L2 normalization ensures ambeddings are unit vectors (important for cosine similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff8453",
   "metadata": {},
   "source": [
    "Text Encoder\n",
    "\n",
    "- Each caption has tokens like [CLS] red triangle left.\n",
    "- A token embedding layer converts each word to a 64-D vector.\n",
    "- A positional embedding layer adds position info (like transformer).\n",
    "- MHA after this.\n",
    "- Followed by a linear layer + LayerNorm + L2 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed996ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
