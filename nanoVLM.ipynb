{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a2eb7c",
   "metadata": {},
   "source": [
    "NanoVLM: tiny CLIP-style model trained on synthetic colored-shaped captions.\n",
    "\n",
    "For a given text caption, we have to retrieve the best images from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42717e0f",
   "metadata": {},
   "source": [
    "This is a tiny CLIP-styled Vision language Model. It has 2 separate encoders- one for images and one for text. Both encoder map their inputs into a common embeddingspace of dimension 64 (or other dimension we choose). THe goal is to make matching image-text pairs lie close together in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b790ab",
   "metadata": {},
   "source": [
    "Image Encoder\n",
    "\n",
    "- A small CNN (4 convolutional layers) progressively downsamples the input image.\n",
    "- After the convolution blocks, a global average pooling layer reduces spatial features.\n",
    "- A linear projection maps to the embedding dimension.\n",
    "- Finally, a LayerNorm + L2 normalization ensures ambeddings are unit vectors (important for cosine similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff8453",
   "metadata": {},
   "source": [
    "Text Encoder\n",
    "\n",
    "- Each caption has tokens like [CLS] red triangle left.\n",
    "- A token embedding layer converts each word to a 64-D vector.\n",
    "- A positional embedding layer adds position info (like transformer).\n",
    "- MHA after this.\n",
    "- Followed by a linear layer + LayerNorm + L2 normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c952c",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed996ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Virtual Environments\\PyTorch\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math, random\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78702a3b",
   "metadata": {},
   "source": [
    "VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33757bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMG_SIZE = 32\n",
    "EMBED_DIM = 64\n",
    "ATTENTION_HEADS = 4\n",
    "BATCH_SIZE = 12\n",
    "EPOCHS = 10\n",
    "LR = 3e-4\n",
    "TEMPERATURE = 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7baec2",
   "metadata": {},
   "source": [
    "SYNTHETIC DATASET PROPERTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94436a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray']\n",
    "shapes = ['square', 'circle', 'triangle']\n",
    "positions = ['left', 'center', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0409b",
   "metadata": {},
   "source": [
    "DRAWING IMAGE SHAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0270fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(color, shape, position, img_size = IMG_SIZE):\n",
    "    img = Image.new('RGB', (img_size, img_size), 'white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    margin = 6\n",
    "    w = h = img_size - 2*margin\n",
    "    # Calculate x coordinates\n",
    "    if 'left' in position:\n",
    "        x0 = margin\n",
    "        x1 = margin + w // 2\n",
    "    elif 'top-left' in position:\n",
    "        x0 = margin\n",
    "        x1 = margin + w // 2\n",
    "    elif 'bottom-left' in position:\n",
    "        x0 = margin\n",
    "        x1 = margin + w // 2\n",
    "    elif 'right' in position:\n",
    "        x0 = margin + w // 2\n",
    "        x1 = img_size - margin\n",
    "    elif 'top-right' in position:\n",
    "        x0 = margin + w // 2\n",
    "        x1 = img_size - margin\n",
    "    elif 'bottom-right' in position:\n",
    "        x0 = margin + w // 2\n",
    "        x1 = img_size - margin\n",
    "    else:   # Center or vertical positions\n",
    "        x0 = margin + w // 4\n",
    "        x1 = margin + 3 * w // 4\n",
    "    # Calculate y coordinates\n",
    "    if 'top' in position:\n",
    "        y0 = margin\n",
    "        y1 = margin + h // 2\n",
    "    elif 'top-left' in position:\n",
    "        y0 = margin\n",
    "        y1 = margin + h // 2\n",
    "    elif 'top-right' in position:\n",
    "        y0 = margin\n",
    "        y1 = margin + h // 2\n",
    "    elif 'bottom' in position:\n",
    "        y0 = margin + h // 2\n",
    "        y1 = img_size - margin\n",
    "    elif 'bottom-left' in position:\n",
    "        y0 = margin + h // 2\n",
    "        y1 = img_size - margin\n",
    "    elif 'bottom-right' in position:\n",
    "        y0 = margin + h // 2\n",
    "        y1 = img_size - margin\n",
    "    else:   # Center or horizontal positions\n",
    "        y0 = margin + h // 4\n",
    "        y1 = margin + 3 * h // 4\n",
    "    \n",
    "    if shape == 'square':\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color, outline='black')\n",
    "    elif shape == 'circle':\n",
    "        draw.ellipse([x0, y0, x1, y1], fill=color, outline='black')\n",
    "    else:   # Triangle\n",
    "        draw.polygon([(x0+(x1-x0)//2, y0), (x0, y1), (x1, y1)], fill=color, outline='black')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f8338",
   "metadata": {},
   "source": [
    "CLASS FOR BUILDING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3f73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset():\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "\n",
    "        for c in colors:\n",
    "            for s in shapes:\n",
    "                for p in positions:\n",
    "                    img = draw_sample(c, s, p)\n",
    "                    cap = f\"{c} {s} {p}\"\n",
    "                    \n",
    "                    self.images.append(torch.from_numpy(np.asarray(img)).permute(2, 0, 1).float()/255.0)\n",
    "                    self.captions.append(cap)\n",
    "        \n",
    "        self.vocab, self.word2idx = self.build_vocab(self.captions)\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        words = sorted({w for t in texts for w in t.split()})\n",
    "        vocab = ['[CLS]'] + words\n",
    "        w2i = {w:i for i, w in enumerate(vocab)}\n",
    "        return vocab, w2i\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        toks = [self.word2idx['[CLS]']] + [self.word2idx[w] for w in text.split()]\n",
    "        return torch.tensor(toks, dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.encode_text(self.captions[idx])\n",
    "    \n",
    "    def __len__(self):  return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7e81f",
   "metadata": {},
   "source": [
    "CREATE FULL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27bf068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['[CLS]', 'blue', 'bottom', 'bottom-left', 'bottom-right', 'brown', 'center', 'circle', 'gray', 'green', 'left', 'orange', 'pink', 'purple', 'red', 'right', 'square', 'top', 'top-left', 'top-right', 'triangle', 'yellow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_18540\\3120046610.py:12: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  self.images.append(torch.from_numpy(np.asarray(img)).permute(2, 0, 1).float()/255.0)\n"
     ]
    }
   ],
   "source": [
    "full_ds = ShapesDataset()\n",
    "VOCAB_SIZE = len(full_ds.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "print(full_ds.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5be640",
   "metadata": {},
   "source": [
    "TRAIN-VAL SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd355918",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(full_ds))\n",
    "val_size = len(full_ds) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fc6ae",
   "metadata": {},
   "source": [
    "DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda85e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8f0a0",
   "metadata": {},
   "source": [
    "DISPLAY A SAMPLE DATA POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f092d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs, encoded_caps = next(iter(train_loader))\n",
    "idx = random.randint(0, len(imgs)-1)\n",
    "img = (imgs[idx].permute(1, 2, 0).numpy()*255).astype(np.uint8) # Convert to displayable image\n",
    "\n",
    "# Decode the caption\n",
    "caption_tokens = encoded_caps[idx].tolist()\n",
    "caption = \" \".join([full_ds.vocab[token] for token in caption_tokens if token in range(len(full_ds.vocab))])\n",
    "\n",
    "# Remove the [CLS] token from the displayed caption\n",
    "caption = caption.replace('[CLS]', '')\n",
    "\n",
    "plt.figure(figsize=(2.5, 2.5))\n",
    "plt.imshow(img)\n",
    "plt.title(caption, fontsize = 8)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5755190",
   "metadata": {},
   "source": [
    "IMAGE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a26d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim = EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.projection = nn.Linear(256, embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.projection(x)\n",
    "        x = F.normalize(self.layernorm1(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d4c1e",
   "metadata": {},
   "source": [
    "TEXT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f19ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim = EMBED_DIM, num_heads = ATTENTION_HEADS, vocab_size = VOCAB_SIZE, context_window = 4):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(context_window, embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, toks):\n",
    "        N, L = toks.shape\n",
    "        position_embedding_ids = torch.arange(L, device = toks.device).unsqueeze(0).expand(N, L)\n",
    "        position_embedding_vectors = self.position_embedding(position_embedding_ids)\n",
    "        token_embedding_vectors = self.token_embedding(toks)\n",
    "        x = token_embedding_vectors + position_embedding_vectors\n",
    "        x = self.mha(x, x, x)[0]\n",
    "        x = x[:,0]\n",
    "        x = self.projection(x)\n",
    "        x = F.normalize(self.norm(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101410c",
   "metadata": {},
   "source": [
    "CLIP LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46a85923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_loss(img_emb, text_emb, temperature = TEMPERATURE):\n",
    "    logits = img_emb @ text_emb.T / temperature\n",
    "    targets = torch.arange(img_emb.size(0), device=img_emb.device)\n",
    "    loss_i = F.cross_entropy(logits, targets)\n",
    "    loss_t = F.cross_entropy(logits.T, targets)\n",
    "    return ((loss_i + loss_t) / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0092d9",
   "metadata": {},
   "source": [
    "MODEL, DATA, OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd85e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_enc = ImageEncoder().to(device)\n",
    "txt_enc = TextEncoder().to(device)\n",
    "params = list(img_enc.parameters()) + list(txt_enc.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872247bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(t, title=None):\n",
    "    img = (t.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "    plt.figure(figsize=(2.2,2.2))\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title, fontsize=8)\n",
    "    plt.imshow(img); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfc18b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Select a random index\u001b[39;00m\n\u001b[32m      4\u001b[39m     random_idx= random.randrange(\u001b[38;5;28mlen\u001b[39m(full_ds))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     sample_img, sample_toks, sample_cap = full_ds[random_idx]\n\u001b[32m      6\u001b[39m     sample_img = sample_img.unsqueeze().to(device)\n\u001b[32m      7\u001b[39m     pre_train_img_emb = img_enc(sample_img).squeeze().cpu().numpy()\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# img_enc.eval(); txt_enc.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Select a random index\n",
    "#     random_idx= random.randrange(len(full_ds))\n",
    "#     sample_img, sample_toks, sample_cap = full_ds[random_idx]\n",
    "#     sample_img = sample_img.unsqueeze().to(device)\n",
    "#     pre_train_img_emb = img_enc(sample_img).squeeze().cpu().numpy()\n",
    "#     pre_train_txt_emb = txt_enc(sample_toks).squeeze().cpu().numpy()\n",
    "#     sample_toks = sample_toks.unsqueeze().to(device)\n",
    "    \n",
    "# # Display the sample image and caption\n",
    "# print(f\"Sample image and caption for embeddings visualization: {sample_cap}\")\n",
    "# show_image(sample_img.squeeze().cpu())\n",
    "# # Function to visualize embeddings (simplified)\n",
    "# def plot_embedding(embedding, title):\n",
    "#     plt.figure(figsize=(8, 1))\n",
    "#     plt.imshow(embedding.reshape(1, -1), aspect = 'auto', cmap='viridis')\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_embedding(pre_train_img_emb, \"Pre-Training Image Embedding\")\n",
    "\n",
    "# plot_embedding(pre_train_txt_emb, \"Pre-Training Text Embedding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
